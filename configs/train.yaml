# FlowFix Full-Scale Training Configuration
# Dataset: ~18,000 PDBs × 60 poses = ~1.1M training samples

device: cuda
seed: 42

# Data - Full Dataset (Dynamic Pose Sampling)
data:
  data_dir: train_data        # ✅ Relative path to current directory
  split_file: null            # Use automatic 80/10/10 split
  max_train_samples: 5000     # ✅ Use all ~18,000 PDBs
  max_val_samples: 100        # ✅ Use 200 validation PDBs
  num_workers: 8              # ✅ Increased for full dataset
  loading_mode: lazy          # lazy, preload, hybrid
  # lazy: Load data from disk on-the-fly (memory efficient, slower)
  # preload: Load all data into memory at init (fast, memory intensive ~180GB for 18k PDBs)
  # hybrid: Preload proteins only, lazy load ligands (balanced, ~50GB)
  # Note: Dynamic dataset samples 1 pose per PDB each epoch (natural augmentation)

# Model dimensions (Optimized from overfit_test)
model:
  # Protein network
  protein_input_scalar_dim: 76
  protein_input_vector_dim: 31
  protein_input_edge_scalar_dim: 39
  protein_input_edge_vector_dim: 8
  protein_hidden_scalar_dim: 64
  protein_hidden_vector_dim: 16
  protein_output_scalar_dim: 64
  protein_output_vector_dim: 16
  protein_num_layers: 4

  # Ligand network
  ligand_input_scalar_dim: 122
  ligand_input_edge_scalar_dim: 44
  ligand_hidden_scalar_dim: 64
  ligand_hidden_vector_dim: 16
  ligand_output_scalar_dim: 64
  ligand_output_vector_dim: 16
  ligand_num_layers: 4

  # Interaction network (CRITICAL: optimized settings)
  interaction_hidden_dim: 256
  interaction_num_heads: 8
  interaction_num_rbf: 32      # ✅ Rich distance encoding
  interaction_pair_dim: 64     # ✅ Expressive pair bias (from overfit_test)
  interaction_num_layers: 4    # ✅ Balanced capacity (~13M params)

  # Velocity predictor
  velocity_hidden_scalar_dim: 64
  velocity_hidden_vector_dim: 16
  velocity_num_layers: 4

  dropout: 0.1                 # ✅ Regularization for large dataset

# Training - Optimized for Full Dataset
training:
  num_epochs: 5000              # ✅ Full dataset needs fewer epochs
  batch_size: 8               # ✅ Balanced for GPU memory (A100/V100)
  num_timesteps_per_sample: 16  # ✅ 8 timesteps per pose (efficient)
  val_batch_size: 8            # ✅ Validation batch size
  learning_rate: 0.0005        # ✅ 5e-4 for stable large-scale training
  gradient_clip: 100.0           # ✅ Reduced from 100.0 for stability
  gradient_accumulation_steps: 4  # ✅ Effective batch = 16 × 8 × 2 = 256
  distance_geometry_weight: 0.1  # ✅ Weight for distance geometry constraint loss
  # Effective batch size = batch_size × num_timesteps × accum_steps = 16 × 8 × 2 = 256

  optimizer:
    type: adam
    weight_decay: 0.0          # ✅ No weight decay (dropout is sufficient)
    betas: [0.9, 0.999]
    eps: 1.0e-08

  scheduler:
    eta_max: 0.003             # Maximum LR (1e-3)
    T_0: 20                    # First cycle length (50 epochs)
    T_mult: 1                  # Keep constant cycle length
    T_up: 5                   # Warmup period (10 epochs)
    gamma: 0.95                # LR decay per cycle (5% decay)

  validation:
    frequency: 5               # ✅ Validate every 2 epochs (large dataset)
    save_best: true
    early_stopping_patience: 100  # ✅ Increased patience for full training

# Sampling (Inference/Validation)
sampling:
  num_steps: 40
  method: euler  # euler, rk4
  schedule: quadratic  # uniform, quadratic, root, sigmoid
  # Uniform: evenly spaced (dt=constant)
  # Quadratic: dense sampling near t=1 (small dt at late, large dt at early) - RECOMMENDED
  # Root: similar to quadratic but gentler
  # Sigmoid: very dense near t=1 (extreme)

# Training timestep sampling
timestep_sampling:
  mu: 0.8              # Mean of logistic-normal (before sigmoid)
  sigma: 1.7           # Std of logistic-normal
  mix_ratio: 0.98      # 98% logistic-normal + 2% uniform (following paper)

# Experiment management (Unified save structure)
experiment:
  base_dir: save              # Base directory for all experiments
  # run_name will be auto-generated as: flowfix_YYYYMMDD_HHMMSS

# Checkpoints
checkpoint:
  save_freq: 10               # Save checkpoint every N epochs
  save_latest: true           # Always save latest.pt
  keep_last_n: 5              # Keep only last N checkpoints (set to -1 to keep all)

# Visualization
visualization:
  enabled: true                # Enable validation animation
  save_animation: true         # Save GIF animations during validation
  animation_fps: 10            # Frames per second for GIF
  num_samples: 1               # Number of samples to visualize per validation

# WandB logging
wandb:
  enabled: true                # Enable WandB logging
  project: "protein-ligand-flowfix"
  entity: null                 # Your WandB username (optional)
  name: null                   # Run name (auto-generated if null)
  tags: ["flow-matching", "protein-ligand", "se3-equivariant"]
  log_gradients: true          # Log gradient norms (total + module-level)
  log_model_weights: true      # Log parameter statistics (module-level)
  log_animations: true         # Log validation trajectory animations
